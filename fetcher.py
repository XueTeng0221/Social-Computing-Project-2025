import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
from datetime import datetime
from collections import defaultdict
from urllib.parse import quote
import json
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from typing import List, Dict, Optional, Tuple

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TiebaFetcher:
    """
    ç™¾åº¦è´´å§æ•°æ®çˆ¬å–å™¨
    
    åŠŸèƒ½:
        1. çˆ¬å–æŒ‡å®šè´´å§çš„å¸–å­åˆ—è¡¨
        2. çˆ¬å–å¸–å­è¯¦æƒ…é¡µçš„æ‰€æœ‰æ¥¼å±‚ï¼ˆå«å›å¤ï¼‰
        3. çˆ¬å–ç”¨æˆ·ä¸ªäººä¸»é¡µä¿¡æ¯
        4. æ„å»ºç”¨æˆ·å…³ç³»ç½‘ç»œ
    
    æ³¨æ„äº‹é¡¹:
        - éœ€è¦è®¾ç½®åˆé€‚çš„å»¶æ—¶é¿å…å°IP
        - å»ºè®®ä½¿ç”¨ä»£ç†æ± æˆ–Cookieæ± 
        - éƒ¨åˆ†å­—æ®µå¯èƒ½éœ€è¦ç™»å½•æ‰èƒ½è·å–ï¼ˆå¦‚å…³æ³¨/ç²‰ä¸æ•°ï¼‰
    """

    def __init__(self, tieba_name: str, max_pages: int = 5, delay_range: Tuple[int, int] = (3, 7)):
        """
        å‚æ•°:
            tieba_name: è´´å§åç§°ï¼ˆå¦‚ "python"ï¼‰
            max_pages: çˆ¬å–å¸–å­åˆ—è¡¨çš„æœ€å¤§é¡µæ•°
            delay_range: è¯·æ±‚é—´éš”éšæœºå»¶æ—¶èŒƒå›´ï¼ˆç§’ï¼‰- å¢åŠ å»¶è¿Ÿ
        """
        self.tieba_name = tieba_name
        self.max_pages = max_pages
        self.delay_range = delay_range
        
        # ç™¾åº¦è´´å§ URL æ¨¡æ¿
        self.base_url = "https://tieba.baidu.com"
        self.list_url_template = f"{self.base_url}/f?kw={quote(tieba_name)}&pn={{page}}"
        self.thread_url_template = f"{self.base_url}/p/{{tid}}"
        self.user_url_template = f"{self.base_url}/home/main?un={{username}}&fr=pb"
        
        # æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://tieba.baidu.com/',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # åˆ›å»º Session å¯¹è±¡ä»¥ä¿æŒè¿æ¥
        self.session = self._create_session()
        
        # æ•°æ®å­˜å‚¨
        self.posts_data = []
        self.users_data = {}
        self.relations_data = []
        self.seen_posts = set()
        self.seen_users = set()

    def _create_session(self):
        """åˆ›å»ºé…ç½®å¥½çš„ requests Session"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,  # é‡è¯•é—´éš”ï¼š1, 2, 4 ç§’
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session

    def _request_with_retry(self, url: str, max_retries: int = 3):
        """å‘é€HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶ - ä¼˜åŒ–ç‰ˆ"""
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶æ—¶ï¼ˆåçˆ¬è™«ï¼‰
                if attempt > 0:
                    wait_time = random.uniform(5, 10)  # é‡è¯•æ—¶ç­‰å¾…æ›´ä¹…
                    logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    time.sleep(random.uniform(*self.delay_range))
                
                # åŠ¨æ€æ›´æ–° Cookieï¼ˆä½ éœ€è¦æ‰‹åŠ¨ä»æµè§ˆå™¨è·å–æœ€æ–°çš„ï¼‰
                cookies = {
                    'BDUSS': 'DhLMXJEbHJXNmw5M3M5aDhIZ2gwOWpJcmZERlQzbzYzS0MtYWhwWE5pcXR2MmRwSVFBQUFBJCQAAAAAAAAAAAEAAACVpW4fMzgxODAzNjgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAK0yQGmtMkBpR',  # âš ï¸ éœ€è¦æ›´æ–°
                    'STOKEN': 'faa8a44959da177afbc0b78534296fa00cfee756410f30073831aebfe833f02d',  # âš ï¸ éœ€è¦æ›´æ–°
                    'BAIDUID': '5C50BAEC366C84488F2E13C4B2F42881:FG=1',  # å¯é€‰ï¼Œä»æµè§ˆå™¨å¤åˆ¶
                    'TIEBA_SID': 'H4sIAAAAAAAAA9MFAPiz3ZcBAAAA'
                }
                
                logger.info(f"æ­£åœ¨è¯·æ±‚ (å°è¯• {attempt+1}/{max_retries}): {url}")
                
                # å¢åŠ è¶…æ—¶æ—¶é—´
                response = self.session.get(
                    url, 
                    headers=self.headers, 
                    cookies=cookies, 
                    timeout=(10, 30),  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
                    allow_redirects=True
                )
                
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                logger.info(f"âœ“ è¯·æ±‚æˆåŠŸ: {url}")
                return response
                
            except requests.exceptions.Timeout as e:
                logger.warning(f"â± è¶…æ—¶ (å°è¯• {attempt+1}/{max_retries}): {e}")
                
            except requests.exceptions.HTTPError as e:
                logger.warning(f"âŒ HTTPé”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if e.response.status_code == 403:
                    logger.error("è¢«æœåŠ¡å™¨æ‹’ç»è®¿é—®ï¼Œå¯èƒ½éœ€è¦æ›´æ–°Cookieæˆ–æ·»åŠ éªŒè¯ç å¤„ç†")
                    return None
                    
            except Exception as e:
                logger.warning(f"âš  å…¶ä»–é”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {e}")
            
            if attempt == max_retries - 1:
                logger.error(f"ğŸ’¥ æœ€ç»ˆå¤±è´¥: {url}")
                return None
        
        return None

    
    def fetch_thread_list(self):
        """ä½¿ç”¨ Selenium çˆ¬å–ï¼ˆé€‚ç”¨äºåŠ¨æ€åŠ è½½é¡µé¢ï¼‰"""
        options = webdriver.ChromeOptions()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(30)
        
        thread_list = []
        
        try:
            for page in range(self.max_pages):
                pn = page * 50
                url = self.list_url_template.format(page=pn)
                logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page+1} é¡µ: {url}")
                
                driver.get(url)
                
                # ğŸ”§ å¢åŠ ç­‰å¾…æ—¶é—´ + å¤šç­–ç•¥æ£€æŸ¥
                try:
                    # ç­–ç•¥1ï¼šç­‰å¾…å¸–å­åˆ—è¡¨å®¹å™¨
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.ID, "thread_list"))
                    )
                except:
                    # ç­–ç•¥2ï¼šç­‰å¾…ä»»æ„å¸–å­é“¾æ¥
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/p/']"))
                    )
                
                # ğŸ”§ æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(3)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ç 
                if "éªŒè¯" in driver.page_source or "captcha" in driver.current_url:
                    logger.error("âš ï¸ é‡åˆ°éªŒè¯ç ï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†ï¼")
                    input("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
                
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                # ğŸ”§ å¤šç§é€‰æ‹©å™¨ç­–ç•¥
                title_links = soup.select('a.j_th_tit')
                title_links += soup.select('li.j_thread_list.clearfix.thread_item_box')
            
                for i, thread in enumerate(title_links):
                    logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} æ¡å¸–å­...")
                    logger.info(f"å¸–å­æ ‡é¢˜: {thread.text.strip()}")
                    try:
                        # æå–å¸–å­IDï¼ˆä»data-fieldå±æ€§ï¼‰
                        data_field = thread.get('data-field')
                        if data_field:
                            logger.info(f"æ•°æ®å­—æ®µ: {data_field}")
                            thread_info = json.loads(data_field)
                            tid = thread_info.get('id')
                            logger.info(f"æå–åˆ°å¸–å­ID: {tid}")
                        else:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šä»é“¾æ¥æå–
                            link = thread.find('a', class_='j_th_tit')
                            logger.info(f"é“¾æ¥: {link}")
                            if not link:
                                continue
                            tid = re.search(r'/p/(\d+)', link['href'])
                            tid = tid.group(1) if tid else None
                            logger.info(f"æå–åˆ°å¸–å­ID: {tid}")
                        
                        if not tid or tid in self.seen_posts:
                            continue
                        
                        logger.info(f"å¸–å­ID: {tid}")
                        
                        # æå–æ ‡é¢˜
                        title_tag = thread.find('a', class_='j_th_tit')
                        title = title_tag.text.strip() if title_tag else "æ— æ ‡é¢˜"
                        
                        # æå–ä½œè€…
                        author_tag = thread.find('span', class_='tb_icon_author')
                        if not author_tag:
                            author_tag = thread.find('a', class_='frs-author-name')
                        author = author_tag.text.strip() if author_tag else "åŒ¿å"
                        
                        # æå–å›å¤æ•°
                        reply_tag = thread.find('span', class_='threadlist_rep_num')
                        reply_count = int(reply_tag.text.strip()) if reply_tag else 0
                        
                        thread_list.append({
                            'tid': tid,
                            'title': title,
                            'author': author,
                            'reply_count': reply_count,
                            'url': f"{self.base_url}/p/{tid}"
                        })
                        
                        self.seen_posts.add(tid)
                        
                    except Exception as e:
                        logger.warning(f"è§£æå¸–å­å¤±è´¥: {e}")
                        continue
            
        finally:
            driver.quit()
        
        logger.info(f"âœ… å…±è·å– {len(thread_list)} ä¸ªå¸–å­")
        return thread_list
    
    def fetch_thread_detail(self, tid: str, max_floors: int = 50):
        """
        ä½¿ç”¨ Selenium çˆ¬å–å¸–å­è¯¦æƒ…ï¼ˆä¸»æ¥¼ + æ¥¼å±‚å›å¤ï¼‰
        
        å‚æ•°:
            tid: å¸–å­ID
            max_floors: æœ€å¤§çˆ¬å–æ¥¼å±‚æ•°
        
        è¿”å›: List[dict] - æ‰€æœ‰æ¥¼å±‚çš„å†…å®¹
        """
        url = self.thread_url_template.format(tid=tid)
        
        # é…ç½® Selenium
        options = webdriver.ChromeOptions()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(30)
        
        floors = []
        
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–å¸–å­è¯¦æƒ…: {url}")
            driver.get(url)
            
            # ç­‰å¾…æ¥¼å±‚å®¹å™¨åŠ è½½
            try:
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "l_post"))
                )
            except:
                logger.warning(f"å¸–å­ {tid} åŠ è½½è¶…æ—¶")
                return []
            
            # æ»šåŠ¨é¡µé¢åŠ è½½æ‰€æœ‰æ¥¼å±‚
            last_height = driver.execute_script("return document.body.scrollHeight")
            scroll_attempts = 0
            max_scroll_attempts = 5
            
            while scroll_attempts < max_scroll_attempts:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                
                last_height = new_height
                scroll_attempts += 1
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ç 
            if "éªŒè¯" in driver.page_source or "captcha" in driver.current_url:
                logger.error("âš ï¸ é‡åˆ°éªŒè¯ç ï¼")
                input("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
            
            # è·å–é¡µé¢æºç 
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰æ¥¼å±‚å®¹å™¨
            floor_divs = soup.find_all('div', class_='l_post')
            logger.info(f"æ‰¾åˆ° {len(floor_divs)} ä¸ªæ¥¼å±‚")
            
            for idx, floor_div in enumerate(floor_divs[:max_floors]):
                try:
                    # è§£ææ¥¼å±‚çš„ data-field
                    data_field = floor_div.get('data-field')
                    if not data_field:
                        continue
                    
                    floor_info = json.loads(data_field)
                    author_id = floor_info['author']['user_id']
                    author_name = floor_info['author']['user_name']
                    post_id = floor_info['content']['post_id']
                    floor_num = floor_info['content']['post_no']
                    
                    # æå–æ¥¼å±‚å†…å®¹
                    content_div = floor_div.find('div', class_='d_post_content')
                    if content_div:
                        # ç§»é™¤å›¾ç‰‡å’Œæ¢è¡Œæ ‡ç­¾
                        for tag in content_div.find_all(['img', 'br']):
                            tag.decompose()
                        content = content_div.get_text(strip=True)
                    else:
                        content = ""
                    
                    # æå–åª’ä½“é“¾æ¥
                    media_urls = []
                    img_tags = floor_div.find_all('img', class_='BDE_Image')
                    for img in img_tags:
                        img_url = img.get('src') or img.get('data-original')
                        if img_url:
                            media_urls.append(img_url)
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯è½¬å‘
                    is_repost = any(kw in content for kw in ['è½¬å‘', 'åˆ†äº«', 'RT @'])
                    
                    # æå–çˆ¶å¸–å­ID
                    parent_post_id = None
                    quote_div = floor_div.find('div', class_='post-tail-wrap')
                    if quote_div:
                        quote_link = quote_div.find('a', href=re.compile(r'pid=(\d+)'))
                        if quote_link:
                            parent_post_id = re.search(r'pid=(\d+)', quote_link['href']).group(1)
                    
                    floors.append({
                        'post_id': post_id,
                        'content': content,
                        'user_id': author_id,
                        'user_name': author_name,
                        'floor_num': floor_num,
                        'is_repost': is_repost,
                        'parent_post_id': parent_post_id,
                        'media_urls': ','.join(media_urls) if media_urls else None,
                        'thread_id': tid
                    })
                    
                    # è®°å½•ç”¨æˆ·
                    if author_id not in self.seen_users:
                        self.seen_users.add(author_id)
                        self.users_data[author_id] = {
                            'user_id': author_id,
                            'user_name': author_name
                        }
                    
                except Exception as e:
                    logger.warning(f"è§£ææ¥¼å±‚ {idx+1} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ“ æˆåŠŸè§£æ {len(floors)} ä¸ªæ¥¼å±‚")
            
        except Exception as e:
            logger.error(f"çˆ¬å–å¸–å­ {tid} å¤±è´¥: {e}")
        
        finally:
            driver.quit()
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(*self.delay_range))
        
        return floors


    def fetch_user_info(self, username: str):
        """
        ä½¿ç”¨ Selenium çˆ¬å–ç”¨æˆ·ä¸ªäººä¸»é¡µä¿¡æ¯
        
        å‚æ•°:
            username: ç”¨æˆ·å
        
        è¿”å›: dict - ç”¨æˆ·ä¿¡æ¯
        """
        url = self.user_url_template.format(username=quote(username))
        
        # é…ç½® Selenium
        options = webdriver.ChromeOptions()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        # å¯é€‰ï¼šæ·»åŠ  Cookieï¼ˆå¦‚æœæœ‰ç™»å½•ä¿¡æ¯ï¼‰
        # options.add_argument('--user-data-dir=C:/Users/YourName/AppData/Local/Google/Chrome/User Data')
        
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(30)
        
        user_info = {}
        
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–ç”¨æˆ·ä¸»é¡µ: {username}")
            driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "userinfo_head"))
                )
            except:
                logger.warning(f"ç”¨æˆ· {username} ä¸»é¡µåŠ è½½è¶…æ—¶")
                return None
            
            time.sleep(2)  # é¢å¤–ç­‰å¾…åŠ¨æ€å†…å®¹
            
            # æ£€æŸ¥éªŒè¯ç 
            if "éªŒè¯" in driver.page_source or "captcha" in driver.current_url:
                logger.error("âš ï¸ é‡åˆ°éªŒè¯ç ï¼")
                input("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
            
            # è·å–é¡µé¢æºç 
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ³¨å†Œæ—¶é—´
            reg_time_tag = soup.find('span', class_='userinfo_title', string=re.compile('æ³¨å†Œæ—¶é—´'))
            if reg_time_tag:
                reg_time = reg_time_tag.find_next('span', class_='userinfo_cont').text.strip()
                user_info['reg_time'] = reg_time
            else:
                user_info['reg_time'] = None
            
            # æå–å‘å¸–æ•°
            post_count_tag = soup.find('span', class_='concern_num', attrs={'id': re.compile('post')})
            if not post_count_tag:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä»å…¶ä»–ä½ç½®æå–
                post_count_tag = soup.find('span', string=re.compile('å‘è´´'))
                if post_count_tag:
                    post_count_text = post_count_tag.find_next('span').text.strip()
                    user_info['post_count'] = int(re.sub(r'\D', '', post_count_text))
                else:
                    user_info['post_count'] = 0
            else:
                user_info['post_count'] = int(post_count_tag.text.strip())
            
            # æå–ç²‰ä¸æ•°å’Œå…³æ³¨æ•°
            fans_tag = soup.find('span', class_='concern_num', attrs={'id': re.compile('fans')})
            follow_tag = soup.find('span', class_='concern_num', attrs={'id': re.compile('concern')})
            
            user_info['follower_count'] = int(fans_tag.text.strip()) if fans_tag else 0
            user_info['following_count'] = int(follow_tag.text.strip()) if follow_tag else 0
            
            # æå–æ˜¯å¦è®¤è¯
            verified_tag = soup.find('img', class_='userinfo_auth')
            user_info['verified'] = bool(verified_tag)
            
            # æå–æ˜¯å¦æœ‰å¤´åƒ
            avatar_tag = soup.find('img', class_='userinfo_head')
            has_avatar = False
            if avatar_tag and avatar_tag.get('src'):
                has_avatar = 'default' not in avatar_tag['src'].lower()
            user_info['has_avatar'] = has_avatar
            
            logger.info(f"âœ“ æˆåŠŸè§£æç”¨æˆ· {username}")
            
        except Exception as e:
            logger.error(f"çˆ¬å–ç”¨æˆ· {username} å¤±è´¥: {e}")
            return None
        
        finally:
            driver.quit()
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(*self.delay_range))
        
        return user_info
    
    def build_user_relations(self, posts_df: pd.DataFrame):
        """
        åŸºäºäº’åŠ¨è¡Œä¸ºæ„å»ºç”¨æˆ·å…³ç³»ç½‘ç»œ
        
        è§„åˆ™:
            1. å›å¤å…³ç³» -> interact
            2. åœ¨åŒä¸€å¸–å­å¤šæ¬¡äº’åŠ¨ -> interactï¼ˆå¼ºåŒ–ï¼‰
            3. ï¼ˆå¯æ‰©å±•ï¼‰åŸºäºå…±åŒå…³æ³¨çš„å§ã€ç›¸ä¼¼æ–‡æœ¬æ¨æ–­æ½œåœ¨å…³ç³»
        
        å‚æ•°:
            posts_df: DataFrame - åŒ…å« user_id, parent_post_id ç­‰å­—æ®µ
        
        è¿”å›: List[dict] - å…³ç³»åˆ—è¡¨
        """
        relations = []
        interaction_count = defaultdict(int)  # {(user1, user2): count}
        
        # æ„å»º post_id -> user_id çš„æ˜ å°„
        post_user_map = dict(zip(posts_df['post_id'], posts_df['user_id']))
        
        for _, row in posts_df.iterrows():
            if pd.notna(row['parent_post_id']) and row['parent_post_id'] in post_user_map:
                # å½“å‰ç”¨æˆ·å›å¤äº†æŸä¸ªå¸–å­ -> å»ºç«‹ interact å…³ç³»
                source_user = row['user_id']
                target_user = post_user_map[row['parent_post_id']]
                
                if source_user != target_user:  # é¿å…è‡ªç¯
                    interaction_count[(source_user, target_user)] += 1
        
        # è½¬æ¢ä¸ºå…³ç³»åˆ—è¡¨ï¼ˆåªä¿ç•™äº’åŠ¨æ¬¡æ•° >= 1 çš„å…³ç³»ï¼‰
        for (src, tgt), count in interaction_count.items():
            relations.append({
                'source_user_id': src,
                'target_user_id': tgt,
                'relation_type': 'interact',
                'interaction_count': count  # å¯ç”¨äºè¾¹æƒé‡
            })
        
        # ï¼ˆå¯é€‰ï¼‰æ·»åŠ  follow å…³ç³»
        # è¿™éœ€è¦çˆ¬å–ç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨ï¼Œç™¾åº¦è´´å§éœ€è¦ç™»å½•ï¼Œæ­¤å¤„çœç•¥
        
        return relations
    
    def run(self):
        """
        æ‰§è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹
        
        è¿”å›: (df_posts, df_users, df_relations)
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–è´´å§: {self.tieba_name}")
        
        # 1. çˆ¬å–å¸–å­åˆ—è¡¨
        thread_list = self.fetch_thread_list()
        
        # 2. çˆ¬å–æ¯ä¸ªå¸–å­çš„è¯¦æƒ…
        all_posts = []
        for thread in thread_list[:10]:  # é™åˆ¶çˆ¬å–æ•°é‡ï¼Œé¿å…è¿‡é•¿æ—¶é—´
            logger.info(f"æ­£åœ¨çˆ¬å–å¸–å­: {thread['title']} (ID: {thread['tid']})")
            floors = self.fetch_thread_detail(thread['tid'])
            all_posts.extend(floors)
        
        # 3. æ„å»º DataFrame
        df_posts = pd.DataFrame(all_posts)
        
        # æ·»åŠ ä¼ªæ ‡ç­¾ï¼ˆå®é™…é¡¹ç›®ä¸­éœ€è¦äººå·¥æ ‡æ³¨ï¼‰
        # è¿™é‡Œç”¨ç®€å•è§„åˆ™ï¼šåŒ…å«é«˜é£é™©å…³é”®è¯çš„æ ‡è®°ä¸º 1
        risk_keywords = ['åŒ…å¡', 'å¸¦å•', 'åŠ ç¾¤', 'QQ', 'å¾®ä¿¡', 'usdt']
        df_posts['label'] = df_posts['content'].apply(
            lambda x: 1 if any(kw in str(x) for kw in risk_keywords) else 0
        )
        
        # 4. è¡¥å……ç”¨æˆ·ä¿¡æ¯ï¼ˆçˆ¬å–ä¸ªäººä¸»é¡µï¼‰
        logger.info(f"å¼€å§‹è¡¥å…… {len(self.users_data)} ä¸ªç”¨æˆ·çš„ä¿¡æ¯...")
        for user_id, user_base_info in list(self.users_data.items())[:20]:  # é™åˆ¶æ•°é‡
            username = user_base_info['user_name']
            logger.info(f"çˆ¬å–ç”¨æˆ·: {username}")
            user_detail = self.fetch_user_info(username)
            
            if user_detail:
                self.users_data[user_id].update(user_detail)
            else:
                # å¡«å……é»˜è®¤å€¼
                self.users_data[user_id].update({
                    'reg_time': '2020-01-01',
                    'post_count': 0,
                    'follower_count': 0,
                    'following_count': 0,
                    'verified': False,
                    'has_avatar': True
                })
        
        df_users = pd.DataFrame(list(self.users_data.values()))
        
        # 5. æ„å»ºç”¨æˆ·å…³ç³»ç½‘ç»œ
        relations = self.build_user_relations(df_posts)
        df_relations = pd.DataFrame(relations)
        
        logger.info(f"âœ… çˆ¬å–å®Œæˆ!")
        logger.info(f"  - å¸–å­æ•°: {len(df_posts)}")
        logger.info(f"  - ç”¨æˆ·æ•°: {len(df_users)}")
        logger.info(f"  - å…³ç³»æ•°: {len(df_relations)}")
        
        return df_posts, df_users, df_relations
    
    def save_to_csv(self, df_posts: pd.DataFrame, df_users: pd.DataFrame, df_relations: pd.DataFrame, output_dir: str = 'data/raw/'):
        """ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        df_posts.to_csv(f'{output_dir}/posts.csv', index=False, encoding='utf-8-sig')
        df_users.to_csv(f'{output_dir}/users.csv', index=False, encoding='utf-8-sig')
        df_relations.to_csv(f'{output_dir}/relations.csv', index=False, encoding='utf-8-sig')
        
        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {output_dir}")


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šçˆ¬å– "python" è´´å§
    fetcher = TiebaFetcher(
        tieba_name='ä¸‰è§’æ´²è¡ŒåŠ¨',
        max_pages=5,
        delay_range=(2, 4)
    )
    
    df_posts, df_users, df_relations = fetcher.run()
    
    # æŸ¥çœ‹æ•°æ®
    print("\n===== Posts Sample =====")
    print(df_posts.head())
    print("\n===== Users Sample =====")
    print(df_users.head())
    print("\n===== Relations Sample =====")
    print(df_relations.head())
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    fetcher.save_to_csv(df_posts, df_users, df_relations)
