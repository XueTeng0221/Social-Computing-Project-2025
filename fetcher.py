import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
import os
import argparse
from collections import defaultdict
from urllib.parse import quote
import json
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
argp = argparse.ArgumentParser()
argp.add_argument('--tieba', type=str, default='ä¸‰è§’æ´²è¡ŒåŠ¨é™ªç©', help='è´´å§åç§°')
argp.add_argument('--max-pages', type=int, default=5, help='æœ€å¤§é¡µæ•°')
argp.add_argument('--max-scrolls', type=int, default=10, help='æ¯ä¸ªå¸–å­æœ€å¤§æ»šåŠ¨æ¬¡æ•°')
argp.add_argument('--max-floor', type=int, default=50, help='æ¯ä¸ªå¸–å­æœ€å¤§çˆ¬å–æ¥¼å±‚æ•°')
argp.add_argument('--output', type=str, default='data', help='è¾“å‡ºç›®å½•')
args = argp.parse_args()


class TiebaFetcher:
    """
    ç™¾åº¦è´´å§æ•°æ®çˆ¬å–å™¨
    
    åŠŸèƒ½:
        1. çˆ¬å–æŒ‡å®šè´´å§çš„å¸–å­åˆ—è¡¨
        2. çˆ¬å–å¸–å­è¯¦æƒ…é¡µçš„æ‰€æœ‰æ¥¼å±‚ï¼ˆå«å›å¤ï¼‰
        3. çˆ¬å–ç”¨æˆ·ä¸ªäººä¸»é¡µä¿¡æ¯
        4. æ„å»ºç”¨æˆ·å…³ç³»ç½‘ç»œ
    
    æ³¨æ„äº‹é¡¹:
        - éœ€è¦è®¾ç½®åˆé€‚çš„å»¶æ—¶é¿å…å°IP
        - å»ºè®®ä½¿ç”¨ä»£ç†æ± æˆ–Cookieæ± 
        - éƒ¨åˆ†å­—æ®µå¯èƒ½éœ€è¦ç™»å½•æ‰èƒ½è·å–ï¼ˆå¦‚å…³æ³¨/ç²‰ä¸æ•°ï¼‰
    """

    def __init__(self, tieba_name, max_pages=5, delay_range=(3, 7)):
        """
        å‚æ•°:
            tieba_name: è´´å§åç§°ï¼ˆå¦‚ "python"ï¼‰
            max_pages: çˆ¬å–å¸–å­åˆ—è¡¨çš„æœ€å¤§é¡µæ•°
            delay_range: è¯·æ±‚é—´éš”éšæœºå»¶æ—¶èŒƒå›´ï¼ˆç§’ï¼‰- å¢åŠ å»¶è¿Ÿ
        """
        self.tieba_name = tieba_name
        self.max_pages = max_pages
        self.delay_range = delay_range
        self.base_url = "https://tieba.baidu.com"
        self.list_url_template = f"{self.base_url}/f?kw={quote(tieba_name)}&pn={{page}}"
        self.thread_url_template = f"{self.base_url}/p/{{tid}}"
        self.user_url_template = f"{self.base_url}/home/main?un={{username}}&fr=pb"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://tieba.baidu.com/',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'Proxy-Connection': 'keep-alive'
        }
        self.session = self._create_session()
        self.posts_data = []
        self.users_data = {}
        self.relations_data = []
        self.seen_posts = set()
        self.seen_users = set()
        self.driver = None
        self._init_driver()

    def _create_session(self):
        """åˆ›å»ºé…ç½®å¥½çš„ requests Session"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,  # é‡è¯•é—´éš”ï¼š1, 2, 4 ç§’
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session

    def _request_with_retry(self, url, max_retries=3):
        """å‘é€HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶ - ä¼˜åŒ–ç‰ˆ"""
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    wait_time = random.uniform(5, 10)  # é‡è¯•æ—¶ç­‰å¾…æ›´ä¹…
                    logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    time.sleep(random.uniform(*self.delay_range))
                
                cookies = {
                    'BDUSS': 'DhLMXJEbHJXNmw5M3M5aDhIZ2gwOWpJcmZERlQzbzYzS0MtYWhwWE5pcXR2MmRwSVFBQUFBJCQAAAAAAAAAAAEAAACVpW4fMzgxODAzNjgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAK0yQGmtMkBpR',  # âš ï¸ éœ€è¦æ›´æ–°
                    'STOKEN': 'faa8a44959da177afbc0b78534296fa00cfee756410f30073831aebfe833f02d',  # âš ï¸ éœ€è¦æ›´æ–°
                    'BAIDUID': '5C50BAEC366C84488F2E13C4B2F42881:FG=1',  # å¯é€‰ï¼Œä»æµè§ˆå™¨å¤åˆ¶
                    'TIEBA_SID': 'H4sIAAAAAAAAA9MFAPiz3ZcBAAAA'
                }
                
                logger.info(f"æ­£åœ¨è¯·æ±‚ (å°è¯• {attempt+1}/{max_retries}): {url}")
                response = self.session.get(
                    url, 
                    headers=self.headers, 
                    cookies=cookies, 
                    timeout=(10, 30),  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
                    allow_redirects=True
                )
                
                response.raise_for_status()
                response.encoding = 'utf-8'
                logger.info(f"âœ“ è¯·æ±‚æˆåŠŸ: {url}")
                return response
                
            except requests.exceptions.Timeout as e:
                logger.warning(f"â± è¶…æ—¶ (å°è¯• {attempt+1}/{max_retries}): {e}")
                
            except requests.exceptions.HTTPError as e:
                logger.warning(f"âŒ HTTPé”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if e.response.status_code == 403:
                    logger.error("è¢«æœåŠ¡å™¨æ‹’ç»è®¿é—®ï¼Œå¯èƒ½éœ€è¦æ›´æ–°Cookieæˆ–æ·»åŠ éªŒè¯ç å¤„ç†")
                    return None
                    
            except Exception as e:
                logger.warning(f"âš  å…¶ä»–é”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {e}")
            
            if attempt == max_retries - 1:
                logger.error(f"ğŸ’¥ æœ€ç»ˆå¤±è´¥: {url}")
                return None
        
        return None

    def _init_driver(self):
        """åˆå§‹åŒ– Selenium Driver (å•ä¾‹å¤ç”¨)"""
        logger.info("æ­£åœ¨åˆå§‹åŒ– Chrome Driver...")
        options = webdriver.ChromeOptions()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        try:
            self.driver = webdriver.Chrome(options=options)
            self.driver.set_page_load_timeout(30)
            logger.info("Chrome Driver åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Chrome Driver åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e
    
    def close(self):
        """å…³é—­ Driver"""
        if self.driver:
            logger.info("æ­£åœ¨å…³é—­ Chrome Driver...")
            self.driver.quit()
            self.driver = None
    
    def fetch_thread_list(self):
        """ä½¿ç”¨ Selenium çˆ¬å–ï¼ˆé€‚ç”¨äºåŠ¨æ€åŠ è½½é¡µé¢ï¼‰"""
        thread_list = []
        for page in range(self.max_pages + 1):
            pn = page * 50
            url = self.list_url_template.format(page=pn)
            logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page + 1} é¡µ: {url}")
            self.driver.get(url)
            try:
                WebDriverWait(self.driver, 20).until(
                    EC.presence_of_element_located((By.ID, "thread_list"))
                )
            except:
                WebDriverWait(self.driver, 20).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='/p/']"))
                )
            
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            if "éªŒè¯" in self.driver.page_source or "captcha" in self.driver.current_url:
                logger.error("âš ï¸ é‡åˆ°éªŒè¯ç ï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†ï¼")
                input("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            title_links = soup.select('a.j_th_tit')
            title_links += soup.select('li.j_thread_list.clearfix.thread_item_box')
            for i, thread in enumerate(title_links):
                logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} æ¡å¸–å­...")
                try:
                    data_field = thread.get('data-field')
                    thread_info = json.loads(data_field)
                    tid = thread_info.get('id')
                    logger.info(f"æå–åˆ°å¸–å­ID: {tid}")
                    
                    if not tid or tid in self.seen_posts:
                        continue
                    
                    logger.info(f"å¸–å­ID: {tid}")
                    title_tag = thread.find('a', class_='j_th_tit')
                    title = title_tag.text.strip() if title_tag else "æ— æ ‡é¢˜"
                    author_tag = thread.find('span', class_='tb_icon_author')
                    if not author_tag:
                        author_tag = thread.find('a', class_='frs-author-name')
                    
                    author = author_tag.text.strip() if author_tag else "åŒ¿å"
                    reply_tag = thread.find('span', class_='threadlist_rep_num')
                    reply_count = int(reply_tag.text.strip()) if reply_tag else 0
                    thread_list.append({
                        'tid': tid,
                        'title': title,
                        'author': author,
                        'reply_count': reply_count,
                        'url': f"{self.base_url}/p/{tid}"
                    })
                    
                    self.seen_posts.add(tid)
                    
                except Exception as e:
                    logger.warning(f"è§£æå¸–å­å¤±è´¥: {e}")
                    continue
            
        logger.info(f"âœ… å…±è·å– {len(thread_list)} ä¸ªå¸–å­")
        return thread_list
    
    def fetch_thread_detail(self, tid, max_floors=50, max_scroll_attempts=10):
        """
        ä½¿ç”¨ Selenium çˆ¬å–å¸–å­è¯¦æƒ…ï¼ˆä¸»æ¥¼ + æ¥¼å±‚å›å¤ï¼‰
        
        å‚æ•°:
            tid: å¸–å­ID
            max_floors: æœ€å¤§çˆ¬å–æ¥¼å±‚æ•°
            max_scroll_attempts: æœ€å¤§æ»šåŠ¨å°è¯•æ¬¡æ•°
        
        è¿”å›: List[dict] - æ‰€æœ‰æ¥¼å±‚çš„å†…å®¹
        """
        url = self.thread_url_template.format(tid=tid)
        floors = []
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–å¸–å­è¯¦æƒ…: {url}")
            self.driver.get(url)
            try:
                WebDriverWait(self.driver, 30).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "l_post"))
                )
            except:
                logger.warning(f"å¸–å­ {tid} åŠ è½½è¶…æ—¶")
                return []
            
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts = 0
            while scroll_attempts < max_scroll_attempts:
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                
                last_height = new_height
                scroll_attempts += 1
            
            if "éªŒè¯" in self.driver.page_source or "captcha" in self.driver.current_url:
                logger.error("âš ï¸ é‡åˆ°éªŒè¯ç ï¼")
                input("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            floor_divs = soup.find_all('div', class_='l_post')
            logger.info(f"æ‰¾åˆ° {len(floor_divs)} ä¸ªæ¥¼å±‚")
            for idx, floor_div in enumerate(floor_divs[:max_floors]):
                try:
                    data_field = floor_div.get('data-field')
                    if not data_field:
                        continue
                    
                    floor_info = json.loads(data_field)
                    author_id = floor_info['author']['user_id']
                    author_name = floor_info['author']['user_name']
                    post_id = floor_info['content']['post_id']
                    floor_num = floor_info['content']['post_no']
                    content_div = floor_div.find('div', class_='d_post_content')
                    if content_div:
                        for tag in content_div.find_all(['img', 'br']):
                            tag.decompose()
                        content = content_div.get_text(strip=True)
                    else:
                        content = ""
                    
                    media_urls = []
                    img_tags = floor_div.find_all('img', class_='BDE_Image')
                    for img in img_tags:
                        img_url = img.get('src') or img.get('data-original')
                        if img_url:
                            media_urls.append(img_url)
                    
                    is_repost = any(kw in content for kw in ['è½¬å‘', 'åˆ†äº«', 'RT @'])
                    parent_post_id = None
                    quote_div = floor_div.find('div', class_='post-tail-wrap')
                    if quote_div:
                        quote_link = quote_div.find('a', href=re.compile(r'pid=(\d+)'))
                        if quote_link:
                            parent_post_id = re.search(r'pid=(\d+)', quote_link['href']).group(1)
                    
                    floors.append({
                        'post_id': post_id,
                        'content': content,
                        'user_id': author_id,
                        'user_name': author_name,
                        'floor_num': floor_num,
                        'is_repost': is_repost,
                        'parent_post_id': parent_post_id,
                        'media_urls': ','.join(media_urls) if media_urls else None,
                        'thread_id': tid
                    })
                    
                    if author_id not in self.seen_users:
                        self.seen_users.add(author_id)
                        self.users_data[author_id] = {
                            'user_id': author_id,
                            'user_name': author_name
                        }
                    
                except Exception as e:
                    logger.warning(f"è§£ææ¥¼å±‚ {idx+1} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ğŸ‘ŒæˆåŠŸè§£æ {len(floors)} ä¸ªæ¥¼å±‚")
            
        except Exception as e:
            logger.error(f"çˆ¬å–å¸–å­ {tid} å¤±è´¥: {e}")
        
        finally:
            time.sleep(random.uniform(*self.delay_range))
        
        return floors


    def fetch_user_info(self, username):
        """
        ä½¿ç”¨ Selenium çˆ¬å–ç”¨æˆ·ä¸ªäººä¸»é¡µä¿¡æ¯
        
        å‚æ•°:
            username: ç”¨æˆ·å
        
        è¿”å›: dict - ç”¨æˆ·ä¿¡æ¯
        """
        url = self.user_url_template.format(username=quote(username))
        user_info = {}
        
        def safe_int(element, pattern=None):
            """å®‰å…¨æå–æ•´æ•°ï¼Œæ”¯æŒæ­£åˆ™åŒ¹é…"""
            try:
                if element is None:
                    return 0
                text = element.get_text(strip=True)
                if pattern:
                    match = re.search(pattern, text)
                    return int(match.group(1)) if match else 0
                return int(re.sub(r'\D', '', text)) if text else 0
            except (ValueError, AttributeError):
                return 0
        
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–ç”¨æˆ·ä¸»é¡µ: {username}")
            self.driver.get(url)
            
            try:
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "userinfo_head"))
                )
            except:
                logger.warning(f"ç”¨æˆ· {username} ä¸»é¡µåŠ è½½è¶…æ—¶")
                return None
            
            time.sleep(2)
            
            if "éªŒè¯" in self.driver.page_source or "captcha" in self.driver.current_url:
                logger.error("âš ï¸ é‡åˆ°éªŒè¯ç ï¼")
                input("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯åæŒ‰å›è½¦ç»§ç»­...")
            
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            userdata_div = soup.find('div', class_='userinfo_userdata')
            if userdata_div:
                text_spans = [span.get_text(strip=True) for span in userdata_div.find_all('span') 
                            if 'userinfo_split' not in span.get('class', [])]
                
                logger.info(f"æå–åˆ°çš„ç”¨æˆ·æ•°æ®: {text_spans}")
                
                for span_text in text_spans:
                    if 'å§é¾„' in span_text:
                        age_match = re.search(r'å§é¾„:([\d.]+)å¹´?', span_text)
                        if age_match:
                            user_info['reg_time'] = float(age_match.group(1))
                        break
                
                for span_text in text_spans:
                    if 'å‘è´´' in span_text or 'å‘å¸–' in span_text:
                        post_match = re.search(r'å‘[è´´å¸–]:(\d+)', span_text)
                        if post_match:
                            user_info['post_count'] = int(post_match.group(1))
                        break
            
            user_info.setdefault('reg_time', None)
            user_info.setdefault('post_count', 0)
            concern_nums = soup.find_all('span', class_='concern_num')
            logger.info(f"æ‰¾åˆ° {len(concern_nums)} ä¸ªå…³æ³¨æ•°æ®æ ‡ç­¾")
            if len(concern_nums) >= 2:
                fans_link = concern_nums[0].find('a')
                user_info['follower_count'] = safe_int(fans_link, r'(\d+)')
                follow_link = concern_nums[1].find('a')
                user_info['following_count'] = safe_int(follow_link, r'(\d+)')
            elif len(concern_nums) == 1:
                link = concern_nums[0].find('a')
                user_info['follower_count'] = safe_int(link, r'(\d+)')
                user_info['following_count'] = 0
            else:
                user_info['follower_count'] = 0
                user_info['following_count'] = 0
            
            verified_tag = soup.find('img', class_='userinfo_auth')
            user_info['verified'] = bool(verified_tag)
            avatar_tag = soup.find('img', class_='userinfo_head')
            has_avatar = False
            if avatar_tag and avatar_tag.get('src'):
                has_avatar = 'default' not in avatar_tag['src'].lower()
            
            user_info['has_avatar'] = has_avatar
            logger.info(f"âœ“ æˆåŠŸè§£æç”¨æˆ· {username}: {user_info}")
            
        except Exception as e:
            logger.error(f"çˆ¬å–ç”¨æˆ· {username} å¤±è´¥: {e}", exc_info=True)
            return None
        
        finally:
            time.sleep(random.uniform(*self.delay_range))
        
        return user_info


    
    def build_user_relations(self, posts_df):
        """
        åŸºäºäº’åŠ¨è¡Œä¸ºæ„å»ºç”¨æˆ·å…³ç³»ç½‘ç»œ
        
        è§„åˆ™:
            1. å›å¤å…³ç³» -> interact
            2. åœ¨åŒä¸€å¸–å­å¤šæ¬¡äº’åŠ¨ -> interactï¼ˆå¼ºåŒ–ï¼‰
            3. ï¼ˆå¯æ‰©å±•ï¼‰åŸºäºå…±åŒå…³æ³¨çš„å§ã€ç›¸ä¼¼æ–‡æœ¬æ¨æ–­æ½œåœ¨å…³ç³»
        
        å‚æ•°:
            posts_df: DataFrame - åŒ…å« user_id, parent_post_id ç­‰å­—æ®µ
        
        è¿”å›: List[dict] - å…³ç³»åˆ—è¡¨
        
        todo: è°ƒè¯•ä¿¡æ¯æ˜¾ç¤º relations è¡¨ä¸ºç©ºï¼Œéœ€è¦ä¿®å¤
        """
        relations = []
        interaction_count = defaultdict(int)  # {(user1, user2): count}
        
        # æ„å»º post_id -> user_id çš„æ˜ å°„
        post_user_map = dict(zip(posts_df['post_id'], posts_df['user_id']))
        for _, row in posts_df.iterrows():
            if pd.notna(row['parent_post_id']) and row['parent_post_id'] in post_user_map:
                # å½“å‰ç”¨æˆ·å›å¤äº†æŸä¸ªå¸–å­ -> å»ºç«‹ interact å…³ç³»
                source_user = row['user_id']
                target_user = post_user_map[row['parent_post_id']]
                if source_user != target_user:  # é¿å…è‡ªç¯
                    interaction_count[(source_user, target_user)] += 1
        
        for (src, tgt), count in interaction_count.items():
            relations.append({
                'source_user_id': src,
                'target_user_id': tgt,
                'relation_type': 'interact',
                'interaction_count': count  # å¯ç”¨äºè¾¹æƒé‡
            })
        
        return relations
    
    def run(self):
        """
        æ‰§è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹
        
        è¿”å›: (df_posts, df_users, df_relations)
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–è´´å§: {self.tieba_name}")
        
        try:
            # 1. çˆ¬å–å¸–å­åˆ—è¡¨
            thread_list = self.fetch_thread_list()
            
            # 2. çˆ¬å–æ¯ä¸ªå¸–å­çš„è¯¦æƒ…
            all_posts = []
            for thread in thread_list[:5]:  # é™åˆ¶æ•°é‡
                if thread['tid'] == 1:
                    continue  # è·³è¿‡ç½®é¡¶å¸–
                
                logger.info(f"æ­£åœ¨çˆ¬å–å¸–å­: {thread['title']} (ID: {thread['tid']})")
                floors = self.fetch_thread_detail(thread['tid'], max_floors=args.max_floor, max_scroll_attempts=args.max_scrolls)
                all_posts.extend(floors)
            
            # 3. æ„å»º DataFrame
            df_posts = pd.DataFrame(all_posts)
            risk_keywords = ['åŒ…å¡', 'å¸¦å•', 'åŠ ç¾¤', 'Q', 'V', 'v', 'q', 'å¾®ä¿¡', 'usdt']
            df_posts['label'] = df_posts['content'].apply(
                lambda x: 1 if any(kw in str(x) for kw in risk_keywords) else 0
            )
            
            # 4. è¡¥å……ç”¨æˆ·ä¿¡æ¯ï¼ˆçˆ¬å–ä¸ªäººä¸»é¡µï¼‰
            logger.info(f"å¼€å§‹è¡¥å…… {len(self.users_data)} ä¸ªç”¨æˆ·çš„ä¿¡æ¯...")
            for user_id, user_base_info in list(self.users_data.items()):  # é™åˆ¶æ•°é‡
                username = user_base_info['user_name']
                logger.info(f"çˆ¬å–ç”¨æˆ·: {username}")
                user_detail = self.fetch_user_info(username)
                if user_detail:
                    self.users_data[user_id].update(user_detail)
                else:
                    self.users_data[user_id].update({
                        'reg_time': 0.0,
                        'post_count': 0,
                        'follower_count': 0,
                        'following_count': 0,
                        'verified': False,
                        'has_avatar': True
                    })
            
            df_users = pd.DataFrame(list(self.users_data.values()))
            
            # 5. æ„å»ºç”¨æˆ·å…³ç³»ç½‘ç»œ
            relations = self.build_user_relations(df_posts)
            df_relations = pd.DataFrame(relations)
            logger.info(f"âœ… çˆ¬å–å®Œæˆ!")
            logger.info(f"  - å¸–å­æ•°: {len(df_posts)}")
            logger.info(f"  - ç”¨æˆ·æ•°: {len(df_users)}")
            logger.info(f"  - å…³ç³»æ•°: {len(df_relations)}")
            return df_posts, df_users, df_relations
        
        finally:
            self.close()
    
    def save_to_csv(self, df_posts, df_users, df_relations, output_dir):
        """ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        df_posts.to_csv(f'{output_dir}/posts.csv', index=False, encoding='utf-8-sig')
        df_users.to_csv(f'{output_dir}/users.csv', index=False, encoding='utf-8-sig')
        df_relations.to_csv(f'{output_dir}/relations.csv', index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {output_dir}")


if __name__ == "__main__":
    fetcher = TiebaFetcher(
        tieba_name=args.tieba,
        max_pages=args.max_pages,
        delay_range=(2, 4)
    )
    
    df_posts, df_users, df_relations = fetcher.run()
    print("\n===== Posts Sample =====")
    print(df_posts.head())
    print("\n===== Users Sample =====")
    print(df_users.head())
    print("\n===== Relations Sample =====")
    print(df_relations.head())
    fetcher.save_to_csv(df_posts, df_users, df_relations, f'{args.output}/raw')
