import torch
import pandas as pd
import os
import argparse
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from torch.nn.parameter import UninitializedParameter
from torch_geometric.data import HeteroData
from torch_geometric.data.storage import BaseStorage, GlobalStorage, NodeStorage, EdgeStorage
from models import FraudDetector
from preprocessor import DataPreprocessor
from sklearn.preprocessing import StandardScaler
from trainer import train_epoch, evaluate, WeightedFocalLoss

# --- æ–°å¢ï¼šå¯è§†åŒ–å·¥å…·å‡½æ•° ---
def plot_training_history(history, save_dir):
    """ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ Loss å’Œ æŒ‡æ ‡å˜åŒ–"""
    epochs = range(1, len(history['loss']) + 1)
    
    plt.figure(figsize=(15, 6))
    
    # å­å›¾ 1: Loss å˜åŒ–
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['loss'], 'b-', label='Training Loss')
    plt.title('Training Loss per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)

    # å­å›¾ 2: éªŒè¯é›†æŒ‡æ ‡å˜åŒ–
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['val_f1'], 'r-', label='Val F1')
    plt.plot(epochs, history['val_auc'], 'g--', label='Val AUC')
    plt.plot(epochs, history['val_precision'], 'c:', label='Val Precision')
    plt.plot(epochs, history['val_recall'], 'm:', label='Val Recall')
    plt.title('Validation Metrics per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Score')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, f'training_history_alpha={args.alpha}_gamma={args.gamma}.png')
    plt.savefig(save_path, dpi=300)
    print(f"ğŸ“Š è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")

def plot_confusion_matrix_result(model, data, mask, save_dir, title="Test Confusion Matrix"):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    model.eval()
    with torch.no_grad():
        out = model(data.x_dict, data.edge_index_dict, post_meta=data['post'].meta)
        # è·å–é¢„æµ‹ç»“æœ
        pred = (out[mask] > 0).float().cpu().numpy()
        y_true = data['post'].y[mask].cpu().numpy()
    
    cm = confusion_matrix(y_true, pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Fraud'])
    
    plt.figure(figsize=(8, 6))
    disp.plot(cmap=plt.cm.Blues, values_format='d')
    plt.title(title)
    
    save_path = os.path.join(save_dir, f'confusion_matrix_alpha={args.alpha}_gamma={args.gamma}.png')
    plt.savefig(save_path, dpi=300)
    print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")

def inspect_label_distribution(data):
    """æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒï¼Œè®¡ç®—å»ºè®®çš„ alpha å€¼"""
    y = data['post'].y
    num_pos = y.sum().item()
    num_total = y.size(0)
    num_neg = num_total - num_pos
    
    print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    print(f"  - æ€»æ ·æœ¬æ•°: {num_total}")
    print(f"  - æ¬ºè¯ˆæ ·æœ¬ (Label=1): {num_pos} ({num_pos/num_total:.2%})")
    print(f"  - æ­£å¸¸æ ·æœ¬ (Label=0): {num_neg} ({num_neg/num_total:.2%})")
    
    if num_pos == 0:
        raise ValueError("âŒ æ•°æ®é›†ä¸­æ²¡æœ‰æ­£æ ·æœ¬ï¼ˆæ¬ºè¯ˆæ ·æœ¬ï¼‰ï¼æ¨¡å‹æ— æ³•è®­ç»ƒã€‚")

    suggested_alpha = num_neg / num_total
    print(f"ğŸ’¡ å»ºè®® Focal Loss Alpha: {suggested_alpha:.4f}")
    return suggested_alpha

torch.serialization.add_safe_globals([
    HeteroData, BaseStorage, GlobalStorage, NodeStorage, EdgeStorage, UninitializedParameter
])

def check_and_normalize_data(data: HeteroData):
    print("\nğŸ” æ­£åœ¨æ£€æŸ¥æ•°æ®è´¨é‡...")
    has_nan = False
    for node_type in data.node_types:
        if torch.isnan(data[node_type].x).any():
            has_nan = True
        if torch.isinf(data[node_type].x).any():
            has_nan = True
            
    if has_nan:
        print("âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æ•°å€¼ï¼Œå°è¯•å°†å…¶æ›¿æ¢ä¸º 0...")
        for node_type in data.node_types:
            data[node_type].x = torch.nan_to_num(data[node_type].x, nan=0.0, posinf=1.0, neginf=-1.0)

    if 'user' in data.node_types:
        print("âš–ï¸ å¯¹ User ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–...")
        scaler = StandardScaler()
        user_x = data['user'].x.cpu().numpy()
        user_x = scaler.fit_transform(user_x)
        data['user'].x = torch.tensor(user_x, dtype=torch.float32)
        
    print("âœ… æ•°æ®æ£€æŸ¥ä¸é¢„å¤„ç†å®Œæˆ")
    return data

def prepare_data(force_rebuild) -> HeteroData:
    graph_path = 'data/processed/hetero_graph.pt'
    if not force_rebuild and os.path.exists(graph_path):
        print("ğŸ“‚ åŠ è½½å·²æœ‰å›¾æ•°æ®...")
        data = torch.load(graph_path, weights_only=True)
    else:
        print("ğŸ”¨ æ„å»ºæ–°å›¾...")
        df_posts = pd.read_csv('data/raw/posts.csv')
        df_users = pd.read_csv('data/raw/users.csv')
        df_relations = pd.read_csv('data/raw/relations.csv')
        preprocessor = DataPreprocessor()
        data = preprocessor.build_graph(df_posts, df_users, df_relations)
        os.makedirs('data/processed', exist_ok=True)
        torch.save(data, graph_path)
        print(f"ğŸ’¾ å›¾å·²ä¿å­˜åˆ° {graph_path}")
    return data

def split_dataset(data, train_ratio=0.7, val_ratio=0.15):
    num_posts = data['post'].y.size(0)
    indices = torch.randperm(num_posts)
    train_size = int(train_ratio * num_posts)
    val_size = int(val_ratio * num_posts)
    train_idx = indices[:train_size]
    val_idx = indices[train_size:train_size + val_size]
    test_idx = indices[train_size + val_size:]
    data['post'].train_mask = torch.zeros(num_posts, dtype=torch.bool)
    data['post'].val_mask = torch.zeros(num_posts, dtype=torch.bool)
    data['post'].test_mask = torch.zeros(num_posts, dtype=torch.bool)
    data['post'].train_mask[train_idx] = True
    data['post'].val_mask[val_idx] = True
    data['post'].test_mask[test_idx] = True
    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†: Train {len(train_idx)} | Val {len(val_idx)} | Test {len(test_idx)}")
    return data

if __name__ == "__main__":
    argp = argparse.ArgumentParser(description="è®­ç»ƒå¼‚æ„å›¾è¯ˆéª—æ£€æµ‹æ¨¡å‹")
    argp.add_argument('--alpha', type=float, default=0.7, help='Focal Loss çš„ alpha å‚æ•°')
    argp.add_argument('--gamma', type=float, default=2.0, help='Focal Loss çš„ gamma å‚æ•°')
    argp.add_argument('--force-rebuild', action='store_true', help='å¼ºåˆ¶é‡å»ºå›¾æ•°æ®') # ä¿®æ­£äº† bool å‚æ•°çš„å†™æ³•
    argp.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒçš„æœ€å¤§è½®æ•°')
    argp.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    argp.add_argument('--weight-decay', type=float, default=5e-4, help='æƒé‡è¡°å‡')
    argp.add_argument('--save-dir', type=str, default='models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    args = argp.parse_args()
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(args.save_dir, exist_ok=True)

    data = prepare_data(force_rebuild=args.force_rebuild)
    data = check_and_normalize_data(data)
    suggested_alpha = inspect_label_distribution(data)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    data = data.to(device)
    data = split_dataset(data)
    
    model = FraudDetector(
        hidden_channels=64,
        out_channels=1,
        metadata=data.metadata()
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    criterion = WeightedFocalLoss(alpha=args.alpha, gamma=args.gamma)
    
    print(f"ğŸš€ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # --- åˆå§‹åŒ–å†å²è®°å½•å­—å…¸ ---
    history = {
        'loss': [],
        'val_f1': [],
        'val_auc': [],
        'val_precision': [],
        'val_recall': []
    }

    best_f1 = 0
    for epoch in range(args.epochs):
        loss = train_epoch(model, data, optimizer, criterion)
        val_f1, val_auc, precision, recall = evaluate(model, data, data['post'].val_mask)
        
        # --- è®°å½•æ•°æ® ---
        history['loss'].append(loss)
        history['val_f1'].append(val_f1)
        history['val_auc'].append(val_auc)
        history['val_precision'].append(precision)
        history['val_recall'].append(recall)

        print(f"Epoch {epoch+1:02d}: Loss {loss:.4f} | Val F1 {val_f1:.4f} | Val AUC {val_auc:.4f}")
        
        if val_f1 >= best_f1:
            best_f1 = val_f1
            torch.save(model.state_dict(), f'{args.save_dir}/best_model_alpha={args.alpha}_gamma={args.gamma}.pth')
            print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (F1={best_f1:.4f})")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    
    # --- ç»˜å›¾ï¼šè®­ç»ƒæ›²çº¿ ---
    print("\nğŸ¨ æ­£åœ¨ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    plot_training_history(history, args.save_dir)

    # 4. æµ‹è¯•æœ€ä½³æ¨¡å‹
    print("\nğŸ” åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    model.load_state_dict(torch.load(f'{args.save_dir}/best_model_alpha={args.alpha}_gamma={args.gamma}.pth', weights_only=True))
    test_f1, test_auc, test_precision, test_recall = evaluate(model, data, data['post'].test_mask)
    print(f"ğŸ¯ æµ‹è¯•é›†æ€§èƒ½: F1 {test_f1:.4f} | AUC {test_auc:.4f} | Precision {test_precision:.4f} | Recall {test_recall:.4f}")
    
    # --- ç»˜å›¾ï¼šæ··æ·†çŸ©é˜µ ---
    print("\nğŸ¨ æ­£åœ¨ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    plot_confusion_matrix_result(model, data, data['post'].test_mask, args.save_dir)
